---
title: "R lab assignment 6: Hypothesis Testing"
author: "Avery Rosado"
output: 
    html_document:
      theme: cosmo
      toc: yes
---

## Resources for the lab
* Read section 6.4 of the textbook to learn about the chi-squared test function
* Read section 7.4 of the textbook to learn about logical tests and conditional
* Read the tutorial R_week6.Rmd and work through the exercises


## Part 1: Chi-squared test for independence

In this section you will use the data set generated by your class, which you will find in the file 'categorical_data.csv'. Load it using the function read.csv() as shown below. The data frame contains four variables: Season, Tea, Height, and Continent. The values of these variables are character strings, such as 'Summer (June-August)' and 'I like certain types sometimes'. The chunk below prints out the counts for different values of the four variables.

```{r}
cat_data <- read.csv("categorical_data.csv")
table(cat_data$Continent)
table(cat_data$Tea)
table(cat_data$Height)
table(cat_data$Season)
```


1. Calculate the counts for the two-way data table to test for independence between Tea and Height. Follow the example below to calculate all the values of the data table and assign it to a matrix. Check that your calculations are correct by making sure that the counts in the matrix add up to the total number of observations.

```{r}
Yes_Short <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Height == 'Short')
Yes_Medium <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Height == 'Medium')
Yes_Tall <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Height == 'Tall')

No_Short <- sum(cat_data$Tea == 'Never'&cat_data$Height == 'Short')
No_Medium <- sum(cat_data$Tea == 'Never'&cat_data$Height == 'Medium')
No_Tall <- sum(cat_data$Tea == 'Never'&cat_data$Height == 'Tall')

Sometimes_Short <- sum(cat_data$Tea=='I like certain types sometimes'&cat_data$Height=='Short')
Sometimes_Medium <- sum(cat_data$Tea=='I like certain types sometimes'&cat_data$Height=='Medium')
Sometimes_Tall <- sum(cat_data$Tea=='I like certain types sometimes'&cat_data$Height=='Tall')

table_entries <- c(Yes_Short, Yes_Medium, Yes_Tall, No_Short, No_Medium, No_Tall, Sometimes_Short, Sometimes_Medium, Sometimes_Tall)
TeaHeight_Table <- matrix(table_entries, ncol=3)
print(TeaHeight_Table)
sum(TeaHeight_Table)
```
 
  
Use the R chisq.test() function to peform the chi-squared test to test the independence of the two variables. Report the p-value for the data set and decide whether to reject the hypothesis that height and tea drinking preference are independent at significance levels alpha = 0.05, 0.01, 0.001.

```{r}
chisq.test(TeaHeight_Table)
```

The p-value produced by this test is significantly larger than the set significance levels listed above, therefore the null hypothesis that height and tea drinking preference are independent should not be rejected. 


2. Calculate the counts for the two-way data table to test for independence between Tea and Continent. Because there are very few people in the data set from continents other than North America, let us combine them into two categories: North America and Other. (For efficiency, use the NOT EQUAL operator !=  to test if the value is not North America.) Check that your calculations are correct by making sure that the counts in the matrix add up to the total number of observations.

```{r}
Yes_NA <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Continent=='North America')
Yes_Other <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Continent!='North America')

No_NA <- sum(cat_data$Tea == 'Never'&cat_data$Continent=='North America')
No_Other <- sum(cat_data$Tea == 'Never'&cat_data$Continent!='North America')

Sometimes_NA <- sum(cat_data$Tea == 'I like certain types sometimes'&cat_data$Continent=='North America')
Sometimes_Other <- sum(cat_data$Tea == 'I like certain types sometimes'&cat_data$Continent!='North America')

table_entries2 <- c(Yes_NA, Yes_Other, No_NA, No_Other, Sometimes_NA, Sometimes_Other)
TeaContinent_Table <- matrix(table_entries2, ncol=3)
print(TeaContinent_Table)
sum(TeaContinent_Table)
```


Use the R chisq.test() function to peform the chi-squared test to test the independence of the two variables. Report the p-value for the data set and decide whether to reject the hypothesis that continent and tea drinking preference are independent at significance levels alpha = 0.05, 0.01, 0.001.

```{r}
chisq.test(TeaContinent_Table)
```

P-value recorded in this test is 0.2291, which is once again greater than the proposed significance levels; this means that the Ho for the scenario does not need to be rejected. 


3. Calculate the counts for the two-way data table to test for independence between Tea and Season. Check that your calculations are correct by making sure that the counts in the matrix add up to the total number of observations.

```{r}
Yes_Autumn <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Season == 'Autumn (September - November)')
Yes_Winter <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Season == 'Winter (December - February)')
Yes_Spring <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Season == 'Spring (March - May)')
Yes_Summer <- sum(cat_data$Tea == 'Yes, all the time'&cat_data$Season == 'Summer (June - August)')

No_Autumn <- sum(cat_data$Tea == 'Never'&cat_data$Season == 'Autumn (September - November)')
No_Winter <- sum(cat_data$Tea == 'Never'&cat_data$Season == 'Winter (December - February)')
No_Spring <- sum(cat_data$Tea == 'Never'&cat_data$Season == 'Spring (March - May)')
No_Summer <- sum(cat_data$Tea == 'Never'&cat_data$Season == 'Summer (June - August)')

Sometimes_Autumn <- sum(cat_data$Tea == 'I like certain types sometimes'&cat_data$Season == 'Autumn (September - November)')
Sometimes_Winter <- sum(cat_data$Tea == 'I like certain types sometimes'&cat_data$Season == 'Winter (December - February)')
Sometimes_Spring <- sum(cat_data$Tea == 'I like certain types sometimes'&cat_data$Season == 'Spring (March - May)')
Sometimes_Summer <- sum(cat_data$Tea == 'I like certain types sometimes'&cat_data$Season == 'Summer (June - August)')

table_entries3 <- c(Yes_Autumn, Yes_Winter, Yes_Spring, Yes_Summer, No_Autumn, No_Winter, No_Spring, No_Summer, Sometimes_Autumn, Sometimes_Winter, Sometimes_Spring, Sometimes_Summer)
TeaSeason_Table <- matrix(table_entries3, ncol=3)
print(TeaSeason_Table)
sum(TeaSeason_Table)
```


Use the R chisq.test() function to peform the chi-squared test to test the independence of the two variables. Report the p-value for the data set and decide whether to reject the hypothesis that birth season and tea drinking preference are independent at significance levels alpha = 0.05, 0.01, 0.001.

```{r}
chisq.test(TeaSeason_Table)
```
 
The p-value produced by this test is 0.5316, which is greater than all 3 proposed siginificance levels alpha. This means that the null hypothesis, proposing that bearth season and tea drinking preference are independent, is not to be rejected
 
 
## Part 2: Generating data for independence hypothesis testing

In this section you will generate random numbers to produce a simulated 2 by 2 contingency table. Use the code provided in Part 2 of the R_week6 Tutorial for an example of generating a random data set of numbers of people with disease for a group genotype A and for a group with genotype B.

1. Generate data sets for genotype A and genotype B of 50 patients with 0.3 probability of disease for both data sets. Place the counts into a data matrix and run a chi-squared test on it. Does the test lead to rejection for the independence hypothesis at 0.1 significance level? How about at 0.05? Based on the parameters used to generate the data sets, is the hypothesis actually true? Did the chi-squared test return the correct result or an error at each significance level?
  
```{r}
set.seed(50)
no.patients <- 50   # sample size
probA <-0.3   # probability of disease for genotype A
probB <- 0.3   # probability of disease for genotype B
distributionA <- rbinom(1,no.patients,probA)   # CREATE random number of patients with disease within genotype A
distributionB <- rbinom(1, no.patients,probB)   # CREATE ...within genotype B
print(c(distributionA, distributionB))

table_entries <- c(distributionA, distributionB, 50-distributionA, 50-distributionB)
table_genotype <- matrix(table_entries, ncol=2)
# print(table_genotype)
chisq.test(table_genotype)

```

The test reveals a p-value of 0.6654, which is higher than the proposed significance levels of 0.1 and 0.05. This means that the null independence hypothesis is not rejected at all for this scenario; based on the paramaters used, it is likely that the hypothesis is true given the very high p-value. 

2. Turn the code in question 1 into a function that has the following inputs: number of patients in each group, probability of disease for genotype A, probability of disease for genotype B, which runs the chi-squared tests and return the p-value. Run the function 100 times by calling it inside a for loop  and assign the p-values to a vector (that you need to pre-allocate before the loop.) Report how many of the 100 chi-squared tests result in rejection of the null hypothesis at the 0.1 and 0.05 significance levels using a logical comparison (see tutorial on logical tests for how to do this.) Based on the parameters used to generate the data sets, how many of the test conclusions are errors for each significance level? Explain how this relates to the definition of p-value.

```{r}
# create genotype function 
GFunction <- function(no.patients,probA, probB) {
  distributionA <- rbinom(1,no.patients,probA) 
  distributionB <- rbinom(1, no.patients,probB)
  table_entries <- c(distributionA, distributionB, 50-distributionA, 50-distributionB)
  table_genotype <- matrix(table_entries, ncol=2)
  chisq.result <- chisq.test(table_genotype)
  return(chisq.result$p.value)
}

# call genotype function 100 times via for
no.trials <- 100
p.values <- rep(0, no.trials)   # pre-allocate vector
for(i in 1:no.trials) {
  p.values[i] <- GFunction(no.patients, probA, probB)
}
# Sum of values that reject the null hypothesis at the 0.1 and 0.05 significance levels using a logical comparison
determinant_level.1 <- rep(0, no.trials)
determinant_level.05 <- rep(0, no.trials)
for(i in 1:no.trials) {
  if(p.values[i] < 0.1){
    determinant_level.1[i] <- 1
  }
  else{
    determinant_level.1[i] <- 0
  }
}
print(paste("There are", sum(determinant_level.1), "chi-sqared results that result in rejection of the Ho."))

for(i in 1:no.trials) {
    if(p.values[i] < 0.05){
    determinant_level.05[i] <- 1
  }
  else{
    determinant_level.05[i] <- 0
  }
}
print(paste("There are", sum(determinant_level.05), "chi-sqared results that result in rejection of the Ho."))
```

For this run, there are 6 conclusions that are erroneous for the 0.1 significance level, and 2 that are erroneous for the 0.05 significance level. Each time that the code is run, the number of errors for each level changes slightly, but the number of errors for the 0.1 level is consistently greater than the number of errors for the 0.05 level, and both make up a small portion of the overall number of trials. 

The trend in comparitive number of errors makes sense considering that the lower the alpha significance level for a model, the lower the chance that Ho is rejected incorrectly.


3. Change the two data sets so they are different: for the first data set (with genotype A), let the probability of disease be 0.3 and for the second data set (genotype B) let disease occur with probability 0.5. Run this script again and report how many of the 100 chi-squared tests result in rejecting the null hypothesis at the 0.1 and 0.05 significance level. Based on the parameters used to generate the data sets, how many of the test conclusions are errors for each significance level? Explain how this relates to the definition of p-value.

```{r}
# genotype function 
GFunction <- function(no.patients,probA, probB) {
  distributionA <- rbinom(1,no.patients,probA) 
  distributionB <- rbinom(1, no.patients,probB)
  table_entries <- c(distributionA, distributionB, 50-distributionA, 50-distributionB)
  table_genotype <- matrix(table_entries, ncol=2)
  chisq.result <- chisq.test(table_genotype)
  return(chisq.result$p.value)
}
probA <- 0.3   # New probability of disease occurrence for genotype A
probB <- 0.5   # New probability of disease occurrence for genotype B
# call GFunction x100 
no.trials <- 100
p.values <- rep(0, no.trials)   
for(i in 1:no.trials) {
  p.values[i] <- GFunction(no.patients, probA, probB)
}
# Sum of values that reject the null hypothesis at the 0.1 and 0.05 significance levels using a logical comparison
determinant_level.1 <- rep(0, no.trials)
determinant_level.05 <- rep(0, no.trials)
for(i in 1:no.trials) {
  if(p.values[i] < 0.1){
    determinant_level.1[i] <- 1
  }
  else{
    determinant_level.1[i] <- 0
  }
}
print(paste("There are", sum(determinant_level.1), "chi-sqared results that result in rejection of the Ho."))

for(i in 1:no.trials) {
    if(p.values[i] < 0.05){
    determinant_level.05[i] <- 1
  }
  else{
    determinant_level.05[i] <- 0
  }
}
print(paste("There are", sum(determinant_level.05), "chi-sqared results that result in rejection of the Ho."))
```

For the run observed, 59 chi-squared results lead to a rejection of the null hypothesis for the 0.1 significance level, and 49 for the 0.01 level. As in the model for the former probability values for genotypes A and B, the higher the significance (alpha level), the more likely results are to incorrectly reject the null hypothesis. As probability of disease increases, the number of errors increases; the trend between alpha level and hypothesis rejection remains the same. 

4. Finally, let's make it really easy for the chi-squared test, and generate data from very different distributions. For the first data set (with genotype A), let the probability of disease be 0.2 and for the second data set (genotype B) let disease occur with probability 0.6. Run this script again and report how many of the 100 chi-squared tests result in rejecting the null hypothesis at the 0.1 and 0.05 significance level. Based on the parameters used to generate the data sets, how many of the test conclusions are errors for each significance level? Explain how this relates to the definition of p-value.

```{r}
# genotype function 
GFunction <- function(no.patients,probA, probB) {
  distributionA <- rbinom(1,no.patients,probA) 
  distributionB <- rbinom(1, no.patients,probB)
  table_entries <- c(distributionA, distributionB, 50-distributionA, 50-distributionB)
  table_genotype <- matrix(table_entries, ncol=2)
  chisq.result <- chisq.test(table_genotype)
  return(chisq.result$p.value)
}
probA <- 0.2   # New probability of disease occurrence for genotype A
probB <- 0.6   # New probability of disease occurrence for genotype B
# call GFunction x100 
no.trials <- 100
p.values <- rep(0, no.trials)   
for(i in 1:no.trials) {
  p.values[i] <- GFunction(no.patients, probA, probB)
}
# Sum of values that reject the null hypothesis at the 0.1 and 0.05 significance levels using a logical comparison
determinant_level.1 <- rep(0, no.trials)
determinant_level.05 <- rep(0, no.trials)
for(i in 1:no.trials) {
  if(p.values[i] < 0.1){
    determinant_level.1[i] <- 1
  }
  else{
    determinant_level.1[i] <- 0
  }
}
print(paste("There are", sum(determinant_level.1), "chi-sqared results that result in rejection of the Ho."))

for(i in 1:no.trials) {
    if(p.values[i] < 0.05){
    determinant_level.05[i] <- 1
  }
  else{
    determinant_level.05[i] <- 0
  }
}
print(paste("There are", sum(determinant_level.05), "chi-sqared results that result in rejection of the Ho."))
```

For the run observed, 100 results of the chi-squared results are erroneous for the 0.1 significance level, and 99 are erroneous for the 0.05 level. Is with the two previous models of varying probabilities of disease, the greater the alpha significance level, the more likely it is that results will incorrectly reject the null hypothesis. However, for these high probabilities, the difference in number of errors between the two alpha values is significantly reduced, and even nonexistent depending on the run; this is because the distributions are very different. The  rise in probabilities elevates the number of cases for results that erroneous for both genotypes. 


## Part 3: Effect of prior probability on predictive value of a test

This simulation illustrates the effect of prior probability (prevalence) of disease on the reliability of positive test results. We will simulate this by setting the prevalence, the false positive rate, and false negative rate, and using random number generators to make two random decisions: 
1. does the person have the disease
2. does the test make an error (false positive for a healthy person, false negative for a diseased person)

1. Use the script provided in the last part of the R_week6 tutorial for generating a vector of health/disease status as your template, and generate a vector of linkage status to disease for 1000 people with probability 0.02 of having disease. Each element is generated by comparing a vector of 1000 uniform random numbers to the prevalence, TRUE indicating disease and FALSE indicating healthy. Generate this vector and print out the number of people with disease.

```{r}
num.people <- 1000
random.vec <- runif(num.people)
disease.vec <- runif(num.people)
prev <- 0.02
disease.vec <- random.vec < prev
print(paste("The number of diseased people is:", sum(disease.vec)))
```
  
Now generate a vector of hypothesis testing errors for healthy people and separately a vector of hypothesis testing errors for people with disease. Again use the script in the tutorial where the vector error.vec was generated for a given error rate (TRUE indicating an error and FALSE indicating the correct decision), but with different type 1 and type 2 error rates so two vectors must be calculated by using the two different rates in the logical comparison. Use type 1 error rate of 0.05 and type 2 error rate of 0.1 and generate the error vectors for all 1000 people, and print out the number of TRUE values in each vector.

```{r}
random.vec1 <- runif(num.people)
error1 <- 0.05
error.vec1 <- random.vec1 < error1
print(paste("The number of testing errors:", sum(error.vec1)))

random.vec2 <- runif(num.people)
error2 <- 0.1
error.vec2 <- random.vec2 < error2
print(paste("The number of testing errors:", sum(error.vec2)))

```
  
Note that this is overcounting the actual type 1 and type 2 errors, since a type 1 error only applies to a healthy person, and a type 2 error only applies to a person with disease. Use the logical & operator to calculate how many people are BOTH healthy and for whom the test makes a type 1 error; and calculate how many people are BOTH disease and for whom test makes a type 2 error. Based on this, calculate and print out the Positive Predictive Value of this test (the fraction of true positives out of all positive test results).
 
```{r}
# How many are BOTH healthy and for whome the test makes a type 1 error
print(paste("The number of people that are both healthy and for whom the test makes a type 1 error:", sum(error.vec1 == TRUE&disease.vec == FALSE)))
# How many are BOTH healthy and for whome the test makes a type 2 error
print(paste("The number of people that are both healthy and for whom the test makes a type 2 error:", sum(error.vec2 == TRUE&disease.vec == TRUE)))
# Positive Predictive Value
true_positive <- sum(error.vec2 == FALSE&disease.vec == TRUE)
PPV <- true_positive/sum(disease.vec)
print(paste("PPV of this test:", PPV))
```


2. Combine your scripts from question 1 above to create a function that has the following inputs: number of people, the prevalence of the disease, type 1 error, and type 2 error, and which returns the positive predictive value of the test. Set the prevalence to 0.1, keeping the same type 1 and type 2 error rates, run your function  and report the PPV. Then change the prevalence to 0.001, run your function and report the PPV. How does the prior probability affect the positive predictive value? What implication does this have for testing a large number of people for a disease with low prevalence?

```{r}
### ORIGINAL
# PPVFunction <- function(num.people,prevalence,type1, type2) {
#   random.vec1 <- runif(num.people)
#   prev <- prevalence
#   disease.vec <- random.vec1 < prevalence
#   error1 <- type1
#   error.vec1 <- random.vec1 < error1
#   random.vec2 <- runif(num.people)
#   error2 <- type2
#   error.vec2 <- random.vec2 < error2
#   true_positive <- sum(error.vec2 == FALSE&disease.vec == TRUE)
#   PPV <- true_positive/sum(disease.vec)   # Positive Predictive Value
#  return(PPV)
# }
#prev 0.1
# PPVFunction(1000,0.1,0.05,0.1)  
#prev 0.001
# PPVFunction(1000,0.001,0.05,0.1)

### EDIT
set.seed(21)
num.people <- 1000
random.vec1 <- runif(num.people) 

error1 <- 0.05
error.vec1 <- random.vec1 < error1
print(paste("The no. of Type 1 testing errors is", sum(error.vec1)))

error2 <- 0.1
error.vec2 <- random.vec1 < error2
print(paste("The no. of Type 2 testing errors is", sum(error.vec2)))

PPVFunction <- function(num.people,prevalence,type1, type2) {
  disease.vec <- runif(num.people) < prevalence   # Random vec for disease                    
  error.vec1 <- random.vec1 < type1   # Random vec for Error 1
  error.vec2 <- random.vec1 < type2   # Random vec for Error 2
  true_positive <- sum(error.vec2 == FALSE&disease.vec == TRUE)
  false_positive <- sum(error.vec1 == TRUE&disease.vec == FALSE)                      
  PPV <- true_positive/(true_positive + false_positive)  # Change denomination to the sum of true positive and false positive
  return(PPV)
}
#prev 0.1
PPVFunction(1000,0.1,0.05,0.1)  
#prev 0.001
PPVFunction(1000,0.001,0.05,0.1)
```
  
A trend emerges between prevalnce and PPV where PPV tends to increase drastically as prevalence decreases, hereby making the identification of diseased individuals more accurate; for this particular seed, the PPV value for prevalence 0.001 is roughly 0.02, however for other runs that value can be reduced to 0. 
 
  
3. Let us investigate the effect of changing the type 1 error rate of the test on the PPV. Set the prevalence to 0.01 and report the PPVs for each of the following combinations of type 1 and type 2 errors:

Type 1: 0.05; Type 2: 0.1   (1)
Type 1: 0.1; Type 2: 0.1   (2)
Type 1: 0.2; Type 2: 0.1   (3)
Type 1: 0.01; Type 2: 0.1   (4)
Type 1: 0.001; Type 2: 0.1   (5)

```{r}
# decreasing Type1
#3
print(paste("PPV3:",PPVFunction(1000,0.01,0.2,0.1)))
#2
print(paste("PPV2:",PPVFunction(1000,0.01,0.1,0.1)))
#1 
print(paste("PPV1:",PPVFunction(1000,0.01,0.05,0.1)))
#4
print(paste("PPV4:",PPVFunction(1000,0.01,0.01,0.1)))
#5
print(paste("PPV5:",PPVFunction(1000,0.01,0.001,0.1)))
```

It is clear that as type 1 error increases, PPV increases drastically, with type 1 error of 0.2 resulting in a low of 0.0412371134020619, and type 1 error of 0.001 resulting in a high of 1. 

4. Let us investigate the effect of changing the type 2 error rate of the test on the PPV. Set the prevalence to 0.01 and report the PPVs for each for the following combinations of type 1 and type 2 errors:

Type 1: 0.05; Type 2: 0.1   (1)
Type 1: 0.05; Type 2: 0.2   (2)
Type 1: 0.05; Type 2: 0.3   (3)
Type 1: 0.05; Type 2: 0.05   (4)
Type 1: 0.05; Type 2: 0.01   (5)

```{r}
#decreasing type 2
#3
print(paste("PPV3:",PPVFunction(1000,0.01,0.05,0.3)))
#2
print(paste("PPV2:",PPVFunction(1000,0.01,0.05,0.2)))
#1 
print(paste("PPV1:",PPVFunction(1000,0.01,0.05,0.1)))
#4
print(paste("PPV4:",PPVFunction(1000,0.01,0.05,0.05)))
#5
print(paste("PPV5:",PPVFunction(1000,0.01,0.05,0.01)))
```
PPV's for each of the combinations are printed above.

A singular trend in the resulting PPV does not emerge after running the various type 2 values above. Furthermore, the range of resulting PPV values is significantly smaller than the same range of values for varying type 1 errors. 


```{R}
histogram(~Petal.Length, data = iris, width = 0.04)
bwplot(~Petal.Length, data = iris)
```
