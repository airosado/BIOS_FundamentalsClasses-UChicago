---
title: "R lab 2 for BIOS 20151"
author: "Avery Rosado"
output:
    html_document: default
---

Before starting work:
* Read sections 3.3 and 8.3 of the textbook
* Go through the R week 2 tutorial


## Part 1: Reading in data and fitting functions by eye

In this section you will read in three different data files and assign each data frame a descriptive name, e.g. Alcohol, and then plot and fit relationship between the variables within each data frame. Your task will be to choose the parameters slope and intercept that produce a plot that does a decent job of resembling the data, meaning that the line should pass through the scatterplot and capture its trend (rising or falling) fairly well.


1. Read in the mutations data set found in the file `kong_mutation_data.txt`, using the option `header=TRUE`. It contains three variables: `PatAge` (paternal age in years), `MatAge` (maternal age in years), and `Mutations` (number of de novo mutations). 

a) Print the mean and standard deviation of each variable, and generate histograms of all three variables.

```{r}
KongMutationData <- read.table(file = "kong_mutation_data.txt", header = TRUE)
mean(KongMutationData$PatAge)
mean(KongMutationData$MatAge)
mean(KongMutationData$Mutations)
sd(KongMutationData$PatAge)
sd(KongMutationData$MatAge)
sd(KongMutationData$Mutations)
hist(KongMutationData$PatAge)
hist(KongMutationData$MatAge)
hist(KongMutationData$Mutations)

```


b) Plot `Mutations` as a function `PatAge` using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for `Mutation`, and use `lines()` to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the `sum()` function) of squared differences between the vector `Mutation` and the vector of predicted values and print it out.
  
```{r}
m <- 2
b <- 4
Partition1 <- 15:50
LinearRegression1 <- c(m*Partition1+b)
plot(KongMutationData$PatAge, KongMutationData$Mutations, xlab="Paternal Age (Yrs.)", ylab="No. of Mutations")
lines(Partition1, LinearRegression1)
Residuals1 <- KongMutationData$Mutations - (m*KongMutationData$PatAge+b)
Residuals1Squared <- Residuals1*Residuals1
SumForSquaredResiduals1 <- sum(Residuals1Squared)
print(SumForSquaredResiduals1)
LinearRegression1 <- lm(Mutations ~PatAge, KongMutationData)
print(LinearRegression1)
      
```


c) Make a plot of `Mutations` vs `MatAge` using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for `Mutation`, and use `lines()` to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the `sum()` function) of squared differences between the vector `Mutation` and the vector of predicted values and print it out.
  
```{r}
m1 <- 2
b1 <- 13
Partition2 <- 15:40
LinearRegression2 <- c(m1 * Partition2 + b1)
plot(KongMutationData$MatAge, KongMutationData$Mutations, xlab="Maternal Age (yrs.)", ylab="Mutations", main = "No. Mutations as  Function of Maternal Age")
lines(Partition2, LinearRegression2, col="Red")
Residuals2 <- KongMutationData$Mutations - (m1 * KongMutationData$MatAge + b1)
Residuals2Squared <- Residuals2*Residuals2
SumForSquaredResiduals2 <- sum(Residuals2Squared)
print(SumForSquaredResiduals2)
LinearRegression2 <- lm(Mutations ~MatAge, KongMutationData)
print(LinearRegression2)
```


2. Read in the data set of heart rates before and after exercises found in the file `HR_class.txt` with the option `header=TRUE`. It contains variables `Rest1` (heart rate at rest in bpm), `Rest2` (heart rate at rest in bpm), `Ex1` (heart rate after exercise in bpm), `Ex2` (heart rate after exercise in bpm).

a) Print the mean and standard deviation of each variable, and generate histograms of all four variables.

```{r}
HeartRateData <- read.table(file = "HR_class.txt", header = TRUE)
mean(HeartRateData$Rest1)
mean(HeartRateData$Rest2)
mean(HeartRateData$Ex1)
mean(HeartRateData$Ex2)
sd(HeartRateData$Rest1)
sd(HeartRateData$Rest2)
sd(HeartRateData$Ex1)
sd(HeartRateData$Ex2)

hist(HeartRateData$Rest1)
hist(HeartRateData$Rest2)
hist(HeartRateData$Ex1)
hist(HeartRateData$Ex2)

```
b) Plot `Ex1` as a function of `Rest1` using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for `Ex1`, and use `lines()` to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the `sum()` function) of squared differences between the vector `Ex1` and the vector of predicted values and print it out.

```{r}
m2 = 0.75
b2 = 55
Partition3 = 40:140
# Rest1 is the dependent variable
LinearRegression3 <- c(m2 * Partition3 + b2)
plot(HeartRateData$Rest1, HeartRateData$Ex1, xlab = "Heart Rate at Rest 1 (BPM)", ylab = "Heart Rate following Exercise 1 (BPM)")
lines(Partition3, LinearRegression3)

Residuals3 <- HeartRateData$Ex1 - (m2*HeartRateData$Rest1+b2)
Residuals3Squared <- Residuals3*Residuals3
SumOfSquaredResiduals3 <- sum(Residuals3Squared)

print(SumOfSquaredResiduals3) 

LinearRegression3 <- lm(Ex1 ~ Rest1, HeartRateData)
print(LinearRegression3)
```


c) Plot `Rest2` as a function of `Rest1` using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for `Rest2`, and use `lines()` to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the `sum()` function) of squared differences between the vector `Rest2` and the vector of predicted values and print it out.


```{r}
m3 <- 0.8
b3 <- 15
Partition4 <- 40:130
#Rest1 is the dependent variable
LinearRegression4 <- c(m3*Partition4+b3)
plot(HeartRateData$Rest1, HeartRateData$Rest2, xlab="Heart Rate at Rest 1 (BPM)", ylab="Heart Rate at Rest 2 (BPM)")
lines(Partition4, LinearRegression4)

Residuals4 <-HeartRateData$Rest2 - (m3*HeartRateData$Rest1+b3)
Residuals4Squared <- Residuals4*Residuals4
SumOfSquaredResiduals4 <- sum(Residuals4Squared)
print(SumOfSquaredResiduals4)

LinearRegression4 <- lm(Rest2 ~ Rest1, HeartRateData)
print(LinearRegression4)

```


3. Read in the data for the concentration of alcohol in blood plasma after administration of alcohol in the file `Alcohol_data.txt` with the option `header=TRUE`. It contains variables `Conc` (blood alcohol concentration in micrograms/mL) and the variable `Time` (in hours after consumption).


a)  Plot `Conc` as a function of `Time` using circles (the default type). Assign values to slope and intercept variables, calculate a vector of predicted values for `Conc`, and use `lines()` to add a plot of the predicted values to the data plot. Try different values of the slope and intercept until you find ones that fit the data reasonably well. Calculate the sum (use the `sum()` function) of squared differences between the vector `Conc` and the vector of predicted values and print it out.

```{r}
AlcoholData <- read.table(file = "Alcohol_data.txt", header = TRUE)
m4 <- -0.1
b4 <- 0.6
Partition5 <- 0:10
#Time is the dependent variable
LinearRegression5 <- c(m4*Partition5+b4)
plot(AlcoholData$Time, AlcoholData$Conc, xlab="Time Elapsed", ylab="Alcohol Concentration")
lines(Partition5, LinearRegression5)

Residuals5 <- AlcoholData$Conc - (m4*AlcoholData$Time+b4)
Residuals5Squared <- Residuals5*Residuals5
SumOfSquaredResiduals5 <- sum(Residuals5Squared)
print(SumOfSquaredResiduals5)

LinearRegression5 <- lm(Conc ~ Time, AlcoholData)
print(LinearRegression5)

#The plotted data reveals the lack of linear relationship between axes, making a linear regression subpar for modelling data.
```



b) The variable `Conc` rises and falls, so it would be better to fit it using two separate linear functions. Assign two different variables for slopes and two different for intercepts of two linear functions, and calculate two vectors of predicted values, by splitting up the vector `Time` into the rising and falling subsets. Reminder: use a range of indices (e.g. 1:5) to create a subset of consecutive elements of a vector. Overlay a plot of two linear functions over the data plot, one to match the rise and the second the decay (they should intercept roughly at the peak of the concentration.) Calculate the sums of squared differences between the two subsets of vector `Conc` and the two vectors of predicted values and print the sum of the two sums.
 
```{r}

m5 <- 0.7
b5 <-0.15
Partition6 <- 0:1
LinearRegressionGuess1 <- c(m5*Partition6+b5)
plot(AlcoholData$Time, AlcoholData$Conc, xlab = "Time Elapsed", ylab = "Blood Alcohol Concentration", main = "Blood Alcohol Concentration as a Function of Time")
lines(Partition6, LinearRegressionGuess1)
Residuals6 <- AlcoholData$Conc[1:7] - (m5*AlcoholData$Time[1:7]+b5)
Residuals6Squared <- Residuals6*Residuals6
SumOfResiduals6Squared <- sum(Residuals6Squared)
print(SumOfResiduals6Squared)

LinearRegression6 <- lm(Conc[1:7] ~ Time[1:7], AlcoholData)
print(LinearRegression6)
#Decreasing portion of the graph
m6 <- -.15
b6 <-.9
Partition7 <- 1:6
LinearRegressionGuess2 <- c(m6*Partition7+b6)
plot(AlcoholData$Time, AlcoholData$Conc, xlab="Time Elapsed", ylab="Blood Alcohol Concentration", main = "Blood Alcohol Concentration as a Function of Time")
lines(Partition7, LinearRegressionGuess2)
Residuals7 <- AlcoholData$Conc[8:18] - (m6*AlcoholData$Time[8:18]+b7)
Residuals7Squared <- Residuals7*Residuals7
SumOfResiduals7Squared <- sum(Residuals7Squared)
print(SumOfResiduals7Squared)

LinearRegression7 <- lm(Conc[8:18] ~ Time[8:18], AlcoholData)
print(LinearRegression7)

plot(AlcoholData$Time, AlcoholData$Conc, xlab = "Time Elapsed", ylab="Blood Alcohol Concentration", main = "Blood Alcohol Concentration as a Function of Time")
lines(Partition7, LinearRegressionGuess2)
lines(Partition6, LinearRegressionGuess1)
SumOfResiduals8Squared <- SumOfResiduals7Squared + SumOfResiduals6Squared
print(SumOfResiduals8Squared)
     
```

The sum of squared differences between the two vector subsets is 0.06870919. 



## Part 2: Linear regression
 
In the following questions, you will do the following:
1) calculate the linear regression on the indicated variables in the data set
2) plot the scatterplot and overlay the linear regression line
3) plot the residuals as a function of the explanatory variable
4) answer questions about interpretation of linear regression
 
1. a) Use the data from the file `kong_mutation_data.txt` and perform the tasks stated above for Mutations as a function of PatAge (remember to label your plots and axes!) 

```{r}
KongMutationData<- read.table(file = "kong_mutation_data.txt", header=TRUE)
LinearRegressionMutVP <- lm(Mutations ~ PatAge, KongMutationData)
plot(KongMutationData$PatAge, KongMutationData$Mutations, xlab = "Paternal Age (yrs.)", ylab = "No. of Mutations", main = "Number of Mutations in Relation to Paternal Age")
abline(lm(Mutations ~ PatAge, KongMutationData))
print(LinearRegressionMutVP)

ResidualsMutVP <- c(KongMutationData$Mutations - (2.012*KongMutationData$PatAge+3.562))
plot(KongMutationData$PatAge, ResidualsMutVP, xlab = "Paternal Age (yrs.)", ylab = "No. of Mutations", main = "Residual data")
print(summary(LinearRegressionMutVP))
```

How does the slope compare to the fit your performed by eye in question 1.1a? What does the slope of that line mean? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions?

The slop determined above, 2.012 is very comparable to the estimated slope of 2 from earlier in the assignment. The slope of the line quantifies the relationship between paternal age and number of mutations--specifically, the number of mutations increased per year of increase in paternal age.The fraction of variance explained by the linear relationship can be attributed to the r^2 value, which is 0.6489, meaning that 64.89% of variance is explained by the linear relationship. There are not significant devations from assumptions made regarding the linear nature of the plot of residuals, or the equal distribution of noise about 0. 

 
b) Perform the same tasks as above for `Mutations` as a function of `MatAge` (remember to label your plots and axes!)


```{r}
KongMutationData <- read.table(file = "kong_mutation_data.txt", header = TRUE)
LinearRegressionMvM <- lm(Mutations ~ MatAge, KongMutationData)
plot(KongMutationData$MatAge, KongMutationData$Mutations, xlab = "Maternal Age (years)", ylab = "No. of Mutations", main = "Number of Mutations in Relation to Maternal Age")
abline(lm(Mutations ~ MatAge, KongMutationData))

print(LinearRegressionMvM)
#printing the regression reveals slope and intercept

ResidualsMvM <- (KongMutationData$Mutations - (1.852*KongMutationData$MatAge+12.869))
plot(KongMutationData$MatAge, ResidualsMvM, xlab = "Maternal Age (yrs.)", ylab = "No. Residuals", main = "Maternal Age in Relation to Residuals")
print(summary(LinearRegressionMvM))
                 
```
 
 
How does the slope compare to the fit your performed by eye in question 1.1c? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Is the number of de novo mutations explained better by maternal age or paternal age? Does the plot of residuals show any significant deviations from the assumptions?
  
The estimation for slope in 1.1c was 2, while the calculation for slope above results in a value of 1.852; this difference is not negligeble consideringt the low value for slope in each configuration, however it is small enough to be overlooked. The slope for this model tells indicates the number of maternal de novo mutations per year aged for a mother. Thus, for every year a mother ages, the number of mutations her potential child will have increases by 1.852. The fraction of the variance that is explained by the linear relationship is calculated via the r^2 value, which is 0.4743, meaning that 47.43% of variance can be attributed to the linear relationship. Comparatively, this same relationship for paternal relationship is characterized by a r^2 value of 64.89%. This indicates that a higher proportion of paternal age variance can be attributed to the linear relationship, and paternal age offers a superior explanantion for the number of de novo mutaitons. Assumptions made regarding the linear trend/relationship of the plot, as well as the equal distribution of noise in the y variable around zero the are kept. This is indicated via the distribution of residuals.


2. a) Use the data from the file `HR_class.txt`and perform the tasks stated above for variables `Ex1` as a function of `Rest1` (remember to label your plots and axes!)

```{r}
HeartRateData <- read.table(file = "HR_class.txt", header = TRUE)
LinearRegressionE1vR1 <- lm(Ex1 ~ Rest1, HeartRateData)
plot(HeartRateData$Rest1, HeartRateData$Ex1, xlab = "Heart Rate at Rest Data 1 (BPM)", ylab = "Heart Rate Folllowing Exercise 1 (BPM)", main = "HR Following Exercise 1 as a Function of HR at Rest 1")
abline(lm(Ex1 ~ Rest1, HeartRateData))

print(LinearRegressionE1vR1)
#Values for slope and intercept are given via the above.

ResidualsE1vR1 <- c(HeartRateData$Ex1-(0.7101*HeartRateData$Rest1+54.2192))
plot(HeartRateData$Rest1, ResidualsE1vR1, xlab = "Heart Rate at Rest 1 (BPM)", ylab = "No. Residuals", main = "Residual Data")
print(summary(LinearRegressionE1vR1))
```

How does the slope compare to the fit your performed by eye in question 1.2b? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions?

Estimated slope and calculated slop bear close resemblance, with the former being 0.75 and the latter 0.7101. In this model, slope reveals the linear relationship between resting heart rate 1 and heart rate following exercise 1, each in beats per minute. The numeric value of slope is the ratio of beats per minute of the former to the beats per minute of the latter. As revealed by the value r^2, 0.128, 12.8% of variance in the linear relationship is explained by this model. Furthermore, since the plot is linear and noise is equally distributed for a certain deviation about the linear regression, it can be said that the plot of residuals does not show significant deviation from the assumptions.


b) Perform the tasks stated above for variables `Rest2` as a function of `Rest1` (remember to label your plots and axes!)

```{r}
HeartRateData <- read.table(file = "HR_class.txt", header = TRUE)
LinearRegressionR2vR1 <- lm(Rest2 ~ Rest1, HeartRateData) 
plot(HeartRateData$Rest1, HeartRateData$Rest2, xlab = "Heart Rate at Rest 1 (BPM)", ylab = "Heart Rate at Rest 2 (bpm)",main = "Heart Rate at Rest 2 as a Function of Heart Rate at Rest 1")
abline(lm(Rest2 ~ Rest1, HeartRateData))

print(LinearRegressionR2vR1)
ResidualsRest2VsRest1 <-c (HeartRateData$Rest2 - (.8194*HeartRateData$Rest1+15.5808))
plot(HeartRateData$Rest1,ResidualsRest2VsRest1, main = "Residuals", xlab = "Heart Rate at Rest 1 (bpm)", ylab= "No. Residuals")
print(summary(LinearRegressionR2vR1))
```

How does the slope compare to the fit your performed by eye in question 1.2c? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions?

The estimated slope and the calculated slope bare close resemblance, with estimated slope =0.8 and calculated slop =0.8194. Each slope indicates the relationship between resting heart rate 1 and 2. Specifically, the ratio of beats per minute at heart rate 1 to beats per minute at heart rate 2. The fraction of variance explained by the linear relationship is 0.639, as indicated by the r^2 value. This value indicates that 63.9% of variance in the linear relationship is accounted for by the model. Since the residuals are scattered  around the line residuals=0,  assumptions regarding the relationship being linear and the noise of the model being evenly distributed are accounted for, and there is not significant deviation from the assumptions. 


  
3. a) Use the data from the file `Alcohol_data.txt` and perform the tasks stated above for variables `Conc` as a function of `Time` (remember to label your plots and axes!)

```{r}
AlcoholData <- read.table(file = "Alcohol_Data.txt", header = TRUE)
LinearRegressionConcvTime <- lm(Conc ~ Time, AlcoholData)
plot(AlcoholData$Time, AlcoholData$Conc, xlab = "Time Elapsed (Hrs.)", ylab = "Alcohol Concentration (BAC)", main = "Alcohol Concentration as a Function of Time Elapsed")
abline(lm(Conc ~ Time, AlcoholData))
print(LinearRegressionConcvTime)

ResidualsConcvTime <- c(AlcoholData$Conc - (-0.06124)*AlcoholData$Time+0.62271)
plot(AlcoholData$Time, ResidualsConcvTime, main = "Residuals", xlab = "Time Elapsed (Hrs.)", ylab = "No. Residuals")
print(summary(LinearRegressionConcvTime))
```

How does the slope compare to the fit your performed by eye in question 1.3a? What does the slope of that line tell you? What fraction of the variance is explained by the linear relationship? Does the plot of residuals show any significant deviations from the assumptions?


The slope of fit performed by eye was -0.1, while the slope determined above is -0.06124. The difference is not very significant, but the fact that the slope is a very low quantity means that all deviation bares greater significance. Slope of the line illustrates the blood alcohol concentration as a function of time in hours. The slop quantifies the change in BAC per hour. As indicated by the r^2 value, 0.1399. only 13.99% of variance in the linear relatioship is accounted for by the model. The plot of the residuals does show significant deviations from the assumptions, since the residuals are, for the most part, not scattered around the line r=0. Thus, the linear relationship is ill-defined and noise is not distributed within a specific standard deviation. The linear relationship presumed at the onset of this model--between time and blood alcohol concentration--is highly flawed and should most likely not be regarded as specific. 


b) As you did in question 1.3b, divide the variables `Time` and `Conc` into two subsets, first for the rising portion and second for the falling portion. Make a new plot of the data, compute two linear regressions for the two subsets of the variables (`Conc` and `Time`) and plot the two best-fit lines on the same plot. You don't need to plot the residuals, but it's a fun challenge if you want to do it!

```{r}
AlcoholData <- read.table(file = "Alcohol_data.txt", header = TRUE)
LinearRegressionConcvTimeIncrease <- lm(Conc[1:7] ~ Time[1:7], AlcoholData)
plot(AlcoholData$Time, AlcoholData$Conc, xlab = "Time Elapsed (Hrs.)", ylab = "Alcohol Concentration (BAC)", main = "Alcohol Concentration as a Function of Time")
print(LinearRegressionConcvTimeIncrease)
#Slope and intercept calculated via above

LinearRegressionConcvTimeDecrease <- lm(Conc[8:18] ~ Time[8:18], AlcoholData)
plot(AlcoholData$Time, AlcoholData$Conc, xlab = "Time Elapsed (Hrs.)", ylab = "Alcohol Concentration(BAC)", main = "Alcohol Concentration as a Function of Time Elapsed")
abline(lm(Conc[8:18] ~ Time[8:18], AlcoholData))
abline(lm(Conc[1:7] ~ Time[1:7], AlcoholData))
print(LinearRegressionConcvTimeDecrease)

ResidualsConcvTimeIncrease <- c(AlcoholData$Conc-(0.7137*AlcoholData$Time+0.1417))
print(ResidualsConcvTimeIncrease[1:7])
ResidualsConcvTimeDecrease <- c(AlcoholData$Conc-(-0.1468*AlcoholData$Time+0.9291))
print(ResidualsConcvTimeDecrease[8:18])
#print for values listed in the next line
ResidualsConcvTimeCombination <- c(-0.1417000, 0.0391121, 0.0806379, 0.0914500, 0.0422621, -0.0262121, -0.0854000, -0.0077844, -0.0334156, 0.0011000, 0.0478000, 0.0045000, 0.0079000, -0.0387000, 0.0147000, 0.0181000, -.0151000, 0.0017000)

plot(AlcoholData$Time, ResidualsConcvTimeCombination, main = "Residuals", xlab = "Time Elapsed (Hrs.)", ylab = "No. Residuals")
print(summary(LinearRegressionConcvTimeIncrease))
print(summary(LinearRegressionConcvTimeDecrease))
```
  

How do the two slopes compare to the fit your performed by eye in Part 1.3b? What do the slopes of the lines mean? What fraction of the variance is explained by the two linear relationships? 


The estimated slope for time increasing was 0.7, while the estimed value for time decreasing was -0.15. Each of these values bare close resemblance to the computed values--0.7137 and -0.1468 for increasing and decreasing respectively. Slope quantifies the relationship between time in hours, whether increasing or decreasing, and blood alcohol concentration. Specifically, slope reveals the ratio of time increase or decrease in blood alcohol concentration. The fraction of variance explained by the two linear relationships is given by the two r^2 values. For the increasing regression line, this value is 0.8745, meaning that 87.45% of the variance in the linear relationship between time elapsed and the blood alcohol concentration increased is accounted for by the model. Meanwhile, the r^2 value of 0.9883 for the decreasing regression line means that 98.83% of the variance in the linear relationship between time and decrease in blood alcohol concentration is accounted for by the model. 





